# MSX Data & BI Automation Pipeline

## Overview

This repository contains an end-to-end **Data & BI Automation pipeline** designed for processing **operational / ERP-style data** sourced from Excel-based reports.

The project demonstrates how raw operational data can be:

* Ingested and standardized
* Transformed using business rules and pricing policies
* Stored as curated datasets (single source of truth)
* Used for automated reporting and distribution

The pipeline reflects real-world data workflows commonly found in **manufacturing, logistics, and finance operations**.

---

## High-level Architecture

```
Excel / Operational Reports
        ↓
ETL_DATA_MSX.py
  - Extract: Read batch Excel files
  - Transform: Clean, deduplicate, apply business rules
  - Load: Write curated data to PostgreSQL
        ↓
PostgreSQL (Curated / Master Tables)
        ↓
send_email_MST.py
  - Aggregate data by business entity
  - Generate Statement of Account (SOA)
  - Distribute reports automatically via email
```

The pipeline is executed in **batch mode** and can be scheduled to run daily.

---

## Technology Stack

* **Python 3** – ETL and automation
* **PostgreSQL** – Persistent storage and transformations
* **pandas** – Data processing
* **psycopg2** – Database connectivity
* **Tkinter** – Lightweight UI for file and data-type selection
* **openpyxl** – Excel generation
* **SMTP / MIME** – Automated email delivery

---

## Project Structure

```
.
├── ETL_DATA_MSX.py      # Data ingestion and transformation pipeline
├── send_email_MST.py   # Automated reporting and email distribution
├── README.md
```

---

## Module Description

### 1. ETL_DATA_MSX.py – Data Ingestion & Transformation

**Purpose**

* Ingest shipment and billing data from multiple Excel files
* Standardize and clean raw operational data
* Apply pricing and discount policies dynamically
* Persist curated results into master tables

**Processing Flow**

1. User selects data type (Flash / JNT)
2. One or more Excel files are selected
3. Raw data is inserted into staging tables
4. Duplicate records are removed at database level
5. Data is joined with policy tables
6. Business rules are applied to calculate fees and adjustments
7. Curated data is inserted into master tables

**Key Characteristics**

* Batch-oriented processing
* SQL-based transformations
* Clear separation between staging and curated layers
* Deterministic and repeatable results

---

### 2. send_email_MST.py – Automated Reporting & Distribution

**Purpose**

* Query curated datasets from PostgreSQL
* Aggregate financial metrics by business entity
* Generate Statement of Account (SOA) reports
* Automatically distribute reports via email

**Processing Flow**

1. User selects data type (Flash / JNT)
2. Curated data is queried for the current processing date
3. Records are grouped by sender / warehouse
4. Summary metrics are calculated
5. Excel reports and HTML email content are generated
6. Emails are sent automatically with attachments

**Key Characteristics**

* Fully automated reporting workflow
* Entity-level personalization
* In-memory report generation (no temporary files)

---

## Data Model Overview

### Staging Tables (`msx_data`)

* Store raw operational data ingested from Excel files

### Policy Tables

* Store discount and pricing rules per warehouse or business unit

### Master Tables (`mst_data`)

* Store curated, business-ready data
* Serve as the single source of truth for reporting and BI tools

---

## How to Run

### 1. Install Dependencies

```bash
pip install pandas psycopg2 openpyxl
```

### 2. Run ETL Pipeline

```bash
python ETL_DATA_MSX.py
```

* Select data type
* Select input Excel files

### 3. Run Automated Reporting

```bash
python send_email_MST.py
```

* Select data type
* Reports are generated and emailed automatically

---

## Design Principles

* Separation of raw and curated data layers
* Business logic implemented close to the data
* Automation-first approach to reporting
* Reproducible and auditable transformations

---

## Future Enhancements

* Externalize configuration using environment variables
* Add logging and error handling
* Schedule execution using cron or workflow orchestrators
* Integrate with BI tools (Power BI, Tableau, etc.)
* Implement data quality checks and validations

---

## Notes

This repository focuses on **data processing and automation patterns** rather than UI or application-level concerns. It is intended as a reference implementation for operational data pipelines in ERP-like environments.

---

**Author:** Huy Vu

---

# MSX Data & BI Automation Pipeline (Phiên bản Tiếng Việt)

## Tổng quan

Repository này triển khai một **pipeline Data & BI Automation hoàn chỉnh**, phục vụ xử lý **dữ liệu vận hành theo mô hình ERP** được trích xuất từ các báo cáo Excel.

Mục tiêu của dự án là minh hoạ cách dữ liệu vận hành thô có thể được:

* Thu thập và chuẩn hoá
* Xử lý theo các quy tắc nghiệp vụ (business rules, pricing policy)
* Lưu trữ thành tập dữ liệu đã tinh chỉnh (curated dataset)
* Sử dụng cho báo cáo tự động và phân phối dữ liệu

Pipeline phản ánh các luồng xử lý dữ liệu phổ biến trong **môi trường sản xuất, logistics và tài chính**.

---

## Kiến trúc tổng thể (High-level Architecture)

```
Excel / Báo cáo vận hành
        ↓
ETL_DATA_MSX.py
  - Extract: Đọc dữ liệu theo lô từ nhiều file Excel
  - Transform: Làm sạch, loại trùng, áp dụng quy tắc nghiệp vụ
  - Load: Ghi dữ liệu đã chuẩn hoá vào PostgreSQL
        ↓
PostgreSQL (Bảng Master / Curated)
        ↓
send_email_MST.py
  - Tổng hợp dữ liệu theo đơn vị nghiệp vụ
  - Sinh báo cáo Statement of Account (SOA)
  - Gửi báo cáo tự động qua email
```

Pipeline được thiết kế theo **batch processing**, phù hợp chạy theo ngày hoặc theo chu kỳ định sẵn.

---

## Công nghệ sử dụng

* **Python 3** – ETL và tự động hoá
* **PostgreSQL** – Lưu trữ và xử lý dữ liệu
* **pandas** – Xử lý và biến đổi dữ liệu
* **psycopg2** – Kết nối cơ sở dữ liệu
* **Tkinter** – Giao diện chọn loại dữ liệu và file đầu vào
* **openpyxl** – Sinh file Excel
* **SMTP / MIME** – Gửi email tự động

---

## Cấu trúc dự án

```
.
├── ETL_DATA_MSX.py      # Pipeline ingest & transform dữ liệu
├── send_email_MST.py   # Tự động hoá báo cáo và gửi email
├── README.md
```

---

## Mô tả chi tiết các module

### 1. ETL_DATA_MSX.py – Ingest & Transform dữ liệu

**Mục đích**

* Thu thập dữ liệu vận đơn / billing từ nhiều file Excel
* Chuẩn hoá dữ liệu vận hành thô
* Áp dụng chính sách chiết khấu và quy tắc tính phí
* Ghi dữ liệu đã xử lý vào bảng master

**Luồng xử lý**

1. Người dùng chọn loại dữ liệu (Flash / JNT)
2. Chọn một hoặc nhiều file Excel đầu vào
3. Insert dữ liệu thô vào bảng staging
4. Loại bỏ bản ghi trùng lặp tại tầng database
5. Join với bảng policy
6. Tính toán các khoản phí, điều chỉnh và tổng tiền
7. Ghi dữ liệu hoàn chỉnh vào bảng master

**Đặc điểm kỹ thuật**

* Xử lý theo lô (batch)
* Transform chủ yếu bằng SQL
* Tách rõ staging và curated layer
* Kết quả có thể tái lập và kiểm toán

---

### 2. send_email_MST.py – Báo cáo & phân phối tự động

**Mục đích**

* Truy vấn dữ liệu đã chuẩn hoá từ PostgreSQL
* Tổng hợp số liệu theo từng đơn vị nghiệp vụ
* Sinh báo cáo Statement of Account (SOA)
* Gửi email báo cáo tự động cho các bên liên quan

**Luồng xử lý**

1. Người dùng chọn loại dữ liệu (Flash / JNT)
2. Truy vấn dữ liệu theo ngày xử lý
3. Gom nhóm dữ liệu theo Sender / Warehouse
4. Tính toán các chỉ số tổng hợp
5. Sinh file Excel và nội dung email HTML
6. Gửi email tự động kèm file báo cáo

**Đặc điểm kỹ thuật**

* Tự động hoá hoàn toàn quy trình báo cáo
* Cá nhân hoá báo cáo theo từng đơn vị
* Không sinh file tạm trên ổ đĩa

---

## Tổng quan mô hình dữ liệu

### Bảng Staging (`msx_data`)

* Lưu dữ liệu vận hành thô từ Excel

### Bảng Policy

* Lưu quy tắc chiết khấu và pricing theo đơn vị

### Bảng Master (`mst_data`)

* Lưu dữ liệu đã chuẩn hoá, sẵn sàng cho BI
* Đóng vai trò *single source of truth*

---

## Hướng dẫn chạy dự án

### 1. Cài đặt thư viện

```bash
pip install pandas psycopg2 openpyxl
```

### 2. Chạy pipeline ETL

```bash
python ETL_DATA_MSX.py
```

### 3. Chạy báo cáo và gửi email

```bash
python send_email_MST.py
```

---

## Nguyên tắc thiết kế

* Phân tách rõ dữ liệu thô và dữ liệu đã chuẩn hoá
* Đưa logic nghiệp vụ về gần tầng dữ liệu
* Ưu tiên tự động hoá báo cáo
* Dễ kiểm toán và mở rộng

---

## Định hướng mở rộng

* Tách cấu hình ra biến môi trường
* Bổ sung logging và xử lý lỗi
* Lập lịch chạy tự động (cron / workflow orchestrator)
* Kết nối trực tiếp với công cụ BI
* Bổ sung kiểm soát chất lượng dữ liệu
